# 7. Kosmos 智能体基础 (Agent Foundations)

Kosmos 的基础设施不仅仅是为了存储和检索文档，更重要的是为运行在其之上的、以LLM为核心的智能体（Agents）提供一个健壮、可观测、可管理的运行环境。统一的作业管理系统 (`Job` System) 是这一切的基石。

## 1. 统一作业管理系统 (`Job` Model)

为了管理所有耗时较长、可能失败、且需要复杂上下文的智能体任务，系统引入了统一的 `Job` 数据模型。它充当了这些任务的“状态机”和“控制面板”。

*   **任务定义**: 一个 `Job` 记录代表了一次特定类型的智能体任务，例如：对文档A进行`chunking`，或对资产B进行`asset_analysis`。
    *   `job_type`: 定义了作业的类型 (`document_processing`, `chunking`, `asset_analysis` 等)。
    *   `document_id`, `asset_id`, `knowledge_space_id`: 明确了作业的操作对象和所属范围。
    *   `initiator_id`: 记录了任务的发起者。

*   **状态管理 (`status`)**: `Job` 模型拥有一个明确的状态字段，用于跟踪任务的整个生命周期：
    *   `PENDING`: 任务已创建，等待Worker执行。
    *   `RUNNING`: Worker正在执行任务。
    *   `PAUSED`: 任务因外部依赖（如等待其他作业完成）而暂停，将在未来自动重试。
    *   `COMPLETED`: 任务成功完成。
    *   `FAILED`: 任务执行失败。
    *   `CANCELLED`: 任务被用户取消。

*   **上下文与进度 (`context`, `progress`)**:
    *   `progress`: 用于存储可量化的任务进度，例如 `{'current_line': 1024, 'total_lines': 4096}`。这使得前端可以展示进度条，并且在任务中断后可以从断点处恢复。
    *   `context`: 用于存储任务执行过程中需要传递的复杂上下文信息。

## 2. 任务触发与前置检查 (Pre-flight Checks)

在 `routers/jobs.py` 中，创建新作业的端点展示了系统在正式启动一个昂贵任务前的严谨性。

1.  **权限验证**: 首先验证发起任务的用户是否有权访问目标资源（文档或资产）。
2.  **凭证预检**: 在将任务放入队列**之前**，系统会调用 `AIProviderService` 预先检查知识空间中是否存在任务所需的、有效的AI凭证（例如，Chunking任务需要一个`SLM`类型的凭证）。
    *   **优势**: 这种“前置检查”避免了将一个注定会因为没有凭证而失败的任务放入队列中，浪费计算资源。它实现了“快速失败”（Fail Fast），并能立即向用户提供明确的反馈。

## 3. 健壮的任务执行逻辑

`tasks` 目录下的所有 `actor` (如 `chunk_document_actor`, `analyze_figure_asset_job`) 都遵循一套健壮的设计模式。

*   **依赖驱动**: 在任务正式开始执行前，它可以检查其依赖的其他作业是否已完成。
    *   **示例**: `chunking` 作业会等待其关联文档的所有 `asset_analysis` 作业完成后再开始。
    *   **暂停与重入队**: 如果检测到依赖未就绪，任务不会失败，而是将自己的状态更新为 `PAUSED`，然后**将自身重新放入队列并设置一个延迟**（如60秒）。这是一种优雅的、基于轮询的依赖等待机制。

*   **幂等性设计 (Idempotency)**: 核心的写操作任务被设计为幂等的，以确保任务重试的安全性。
    *   **示例**: `chunk_document_actor` 在开始执行时，会首先删除目标文档所有已存在的 `Chunk` 记录。
    *   **优势**: 这保证了无论任务是首次执行还是因失败重试，它总是在一个干净、一致的状态下开始，从根本上避免了数据污染和重复。

## 4. “LLM即索引器”模式 (LLM as an Indexer)

这是为 `ChunkingAgent` 设计的核心模式，旨在最大限度地提高效率、保真度和经济性。

*   **传统方式的问题**: 让LLM读取文本，然后输出分割后的文本块，存在几个问题：
    1.  **成本高**: 输出的Token与输入几乎一样多，成本高昂。
    2.  **易失真**: LLM在“复制”文本时可能会引入微小的、难以察觉的错误（幻觉）。
    3.  **二义性**: 如果输出的文本块在原文中出现多次，很难确定它到底对应哪一个。

*   **索引器模式**:
    1.  **输入**: 在将文本块发送给LLM之前，我们先为其**添加行号**。
    2.  **任务**: 我们要求LLM不返回文本内容，而是调用我们提供的工具（如 `identify_headings`, `generate_content_summary`），在工具的参数中只提供它识别出的语义块的**起止行号**、**摘要**等元数据。
    3.  **输出**: LLM的输出是一系列的工具调用（Tool Calls），而不是大段的文本。
    4.  **后端处理**: Worker接收到这些工具调用后，根据行号从**原始的、未经修改的**规范化内容中精确地提取出文本，并创建 `Chunk` 记录。

*   **优势**:
    *   **极低成本**: LLM的输出被压缩为结构化的、信息密度极高的工具调用，Token消耗大幅降低。
    *   **100%保真**: `Chunk` 中存储的 `raw_content` 是从源头精确提取的，杜绝了任何形式的LLM幻觉或复制错误。
    *   **无歧义**: 基于行号的引用是绝对精确的。

这个模式是 Kosmos 智能体设计的基石，它将LLM的强大语义理解能力与系统的工程严谨性完美地结合在了一起。
