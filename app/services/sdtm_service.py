import asyncio
import json
import uuid
import logging
from sqlalchemy.orm import Session
from typing import Dict, List, Optional, Any
from datetime import datetime

from app.models.sdtm import (
    SDTMMode, ProgressMetrics, QualityMetrics, 
    DocumentInfo, AbnormalDocument, EditOperation, 
    DocumentAnnotation, SDTMEngineResponse, SDTMStats, SDTMJob
)
from app.db.database import SessionLocal
from app.models.knowledge_base import KnowledgeBase
from app.models.chunk import Chunk
from app.services.sdtm_engine import SDTMEngine
from app.services.sdtm_stats_service import SDTMStatsService
from app.services.kb_service import KBService
from app.repositories.chunk_repo import ChunkRepository

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class SDTMService:
    """SDTMæœåŠ¡ - æ™ºèƒ½æ ‡ç­¾ç”Ÿæˆä¸æ ‡ç­¾å­—å…¸ä¼˜åŒ–ç³»ç»Ÿ
    
    SDTM (Streaming Domain Topic Modeling) çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ›¿ä»£ä¼ ç»Ÿçš„æ–‡æ¡£æ ‡ç­¾ç”Ÿæˆè¿‡ç¨‹
    2. åŸºäºå·²æ‘„å…¥çš„chunksç”Ÿæˆæ™ºèƒ½æ ‡ç­¾
    3. ä¼˜åŒ–å’Œç»´æŠ¤çŸ¥è¯†åº“çš„æ ‡ç­¾å­—å…¸
    4. åŒæ—¶æ›´æ–°SQLiteå’ŒMilvusä¸­çš„æ ‡ç­¾è®°å½•
    
    å·¥ä½œæµç¨‹ï¼š
    - è¯»å–å·²æ‘„å…¥çš„chunksï¼ˆæ— æ ‡ç­¾æˆ–éœ€è¦é‡æ–°æ ‡æ³¨ï¼‰
    - ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½æ ‡ç­¾
    - ä¼˜åŒ–æ ‡ç­¾å­—å…¸ç»“æ„
    - æ›´æ–°chunkçš„æ ‡ç­¾å­—æ®µ
    - åŒæ­¥æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„è®°å½•
    """
    
    def __init__(self, db: Session):
        self.db = db
        self.engine = SDTMEngine()
        self.stats_service = SDTMStatsService(db)
        self.kb_service = KBService(db)
        self.chunk_repo = ChunkRepository(db)
    
    async def start_sdtm_job(
        self, 
        kb_id: str, 
        mode: SDTMMode, 
        batch_size: int = 10, 
        auto_apply: bool = True,
        abnormal_doc_slots: int = 3,
        normal_doc_slots: int = 7,
        max_iterations: int = 50,
        abnormal_doc_threshold: float = 3.0,
        enable_early_termination: bool = True
    ) -> str:
        """å¯åŠ¨SDTMä»»åŠ¡ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            mode: å¤„ç†æ¨¡å¼
            batch_size: æ‰¹å¤„ç†å¤§å°
            auto_apply: æ˜¯å¦è‡ªåŠ¨åº”ç”¨æ“ä½œ
            abnormal_doc_slots: å¼‚å¸¸æ–‡æ¡£å¤„ç†æ§½ä½
            normal_doc_slots: æ­£å¸¸æ–‡æ¡£å¤„ç†æ§½ä½
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            abnormal_doc_threshold: å¼‚å¸¸æ–‡æ¡£é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰
            enable_early_termination: æ˜¯å¦å¯ç”¨æå‰ç»ˆæ­¢
            
        Returns:
            ä»»åŠ¡ID
        """
        # åˆ›å»ºä»»åŠ¡è®°å½•
        job_id = str(uuid.uuid4())
        job = SDTMJob(
            id=job_id,
            kb_id=kb_id,
            mode=mode.value,
            batch_size=batch_size,
            auto_apply=auto_apply,
            status="pending"
        )
        self.db.add(job)
        self.db.commit()

        try:
            # ç¡®ä¿ä»»åŠ¡é˜Ÿåˆ—å·²å¯åŠ¨
            from app.utils.task_queue import task_queue
            if not task_queue._running:
                logger.warning("ä»»åŠ¡é˜Ÿåˆ—æœªè¿è¡Œï¼Œå°è¯•å¯åŠ¨...")
                await task_queue.start()
            
            logger.debug(f"ä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€: running={task_queue._running}, worker_task={task_queue._worker_task}")
            
            # å°†ä»»åŠ¡æ·»åŠ åˆ°å¼‚æ­¥é˜Ÿåˆ—
            task_id = await task_queue.add_task(
                self._run_sdtm_sync,
                job_id, kb_id, mode, batch_size, auto_apply,
                abnormal_doc_slots, normal_doc_slots, max_iterations,
                abnormal_doc_threshold, enable_early_termination,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )

            # æ›´æ–°jobè®°å½•ï¼Œå…³è”task_id
            job.task_id = task_id
            self.db.commit()
            
            logger.info(f"SDTMä»»åŠ¡å·²æ·»åŠ åˆ°é˜Ÿåˆ—: {job_id}, é˜Ÿåˆ—ä»»åŠ¡ID: {task_id}")
            
            return job_id

        except Exception as e:
            # å¦‚æœæ·»åŠ ä»»åŠ¡å¤±è´¥ï¼Œæ›´æ–°ä»»åŠ¡çŠ¶æ€
            job.status = "failed"
            job.error_message = str(e)
            self.db.commit()
            raise e
    
    def _run_sdtm_sync(
        self, 
        job_id: str, 
        kb_id: str, 
        mode: SDTMMode, 
        batch_size: int, 
        auto_apply: bool,
        abnormal_doc_slots: int = 3,
        normal_doc_slots: int = 7,
        max_iterations: int = 50,
        abnormal_doc_threshold: float = 3.0,
        enable_early_termination: bool = True
    ):
        """åŒæ­¥ç‰ˆæœ¬çš„SDTMå¤„ç†ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
        db = SessionLocal()
        try:
            logger.info(f"å¼€å§‹å¤„ç†SDTMä»»åŠ¡: {job_id}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            job = db.query(SDTMJob).filter(SDTMJob.id == job_id).first()
            if not job:
                raise Exception(f"SDTMä»»åŠ¡ä¸å­˜åœ¨: {job_id}")

            job.status = "processing"
            db.commit()
            logger.info(f"SDTMä»»åŠ¡çŠ¶æ€æ›´æ–°ä¸º processing: {job_id}")

            # åˆ›å»ºæ–°çš„æœåŠ¡å®ä¾‹ï¼ˆä½¿ç”¨æ–°çš„æ•°æ®åº“ä¼šè¯ï¼‰
            sdtm_service = SDTMService(db)
            
            # æ‰§è¡ŒSDTMå¤„ç†
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            processing_success = False
            sdtm_result = None
            
            try:
                logger.info("å¼€å§‹æ‰§è¡ŒSDTMå¤„ç†...")
                result = loop.run_until_complete(
                    sdtm_service.process_knowledge_base(
                        kb_id=kb_id,
                        mode=mode,
                        batch_size=batch_size,
                        auto_apply=auto_apply,
                        max_iterations=max_iterations,
                        abnormal_doc_slots=abnormal_doc_slots,
                        normal_doc_slots=normal_doc_slots,
                        abnormal_doc_threshold=abnormal_doc_threshold,
                        enable_early_termination=enable_early_termination
                    )
                )
                sdtm_result = result
                processing_success = True
                logger.info("SDTMå¤„ç†æ‰§è¡ŒæˆåŠŸ")
                
            except Exception as processing_error:
                logger.error(f"SDTMå¤„ç†æ‰§è¡Œå¤±è´¥: {processing_error}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                processing_success = False
                sdtm_result = {
                    "success": False,
                    "message": f"SDTMå¤„ç†å¤±è´¥: {str(processing_error)}",
                    "error": str(processing_error)
                }
            finally:
                loop.close()
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            job = db.query(SDTMJob).filter(SDTMJob.id == job_id).first()
            if job:
                if processing_success:
                    job.status = "completed"
                    logger.info(f"SDTMä»»åŠ¡çŠ¶æ€æ ‡è®°ä¸ºå®Œæˆ: {job_id}")
                else:
                    job.status = "failed"
                    job.error_message = sdtm_result.get("error", "SDTMå¤„ç†å¤±è´¥") if isinstance(sdtm_result, dict) else "SDTMå¤„ç†å¤±è´¥"
                    logger.error(f"SDTMä»»åŠ¡çŠ¶æ€æ ‡è®°ä¸ºå¤±è´¥: {job_id}")
                
                # å°è¯•åºåˆ—åŒ–ç»“æœï¼ˆå¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼‰
                try:
                    if sdtm_result:
                        # ä½¿ç”¨æ–°çš„æœåŠ¡å®ä¾‹è¿›è¡Œåºåˆ—åŒ–æµ‹è¯•
                        if not sdtm_service.test_serialization(sdtm_result):
                            raise Exception("é¢„åºåˆ—åŒ–æµ‹è¯•å¤±è´¥")
                        
                        serializable_result = sdtm_service._make_result_serializable(sdtm_result)
                        job.result = json.dumps(serializable_result, ensure_ascii=False)
                        logger.info(f"SDTMä»»åŠ¡ç»“æœåºåˆ—åŒ–æˆåŠŸ: {job_id}")
                    
                except Exception as serialization_error:
                    logger.warning(f"SDTMä»»åŠ¡ç»“æœåºåˆ—åŒ–å¤±è´¥ï¼ˆä¸å½±å“ä¸»åŠŸèƒ½ï¼‰: {serialization_error}")
                    
                    # æä¾›ç®€åŒ–çš„ç»“æœä¿¡æ¯
                    fallback_result = {
                        "success": processing_success,
                        "message": sdtm_result.get("message", "ä»»åŠ¡å®Œæˆ") if isinstance(sdtm_result, dict) else "ä»»åŠ¡å®Œæˆ",
                        "operations_count": len(sdtm_result.get("operations", [])) if isinstance(sdtm_result, dict) else 0,
                        "annotations_count": len(sdtm_result.get("annotations", [])) if isinstance(sdtm_result, dict) else 0,
                        "serialization_error": f"ç»“æœåºåˆ—åŒ–å¤±è´¥: {str(serialization_error)}",
                        "completed_at": datetime.now().isoformat()
                    }
                    job.result = json.dumps(fallback_result, ensure_ascii=False)
                    logger.warning(f"SDTMä»»åŠ¡ä½¿ç”¨ç®€åŒ–ç»“æœä¿å­˜: {job_id}")
                
                # æäº¤ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆæ— è®ºåºåˆ—åŒ–æ˜¯å¦æˆåŠŸï¼‰
                db.commit()
                logger.info(f"SDTMä»»åŠ¡çŠ¶æ€å·²æäº¤åˆ°æ•°æ®åº“: {job_id}")
                
                # éªŒè¯æ ‡ç­¾å­—å…¸æ˜¯å¦ç¡®å®è¢«æ›´æ–°äº†
                if processing_success:
                    self._verify_tag_dictionary_update(db, kb_id, job_id)

        except Exception as e:
            print(f"ğŸ’¥ SDTMä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {job_id}, é”™è¯¯: {str(e)}")
            import traceback
            print(f"ğŸ’¥ ä¸¥é‡é”™è¯¯çš„è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
            
            # åªåœ¨ç¡®å®éœ€è¦æ—¶æ‰å›æ»šï¼ˆæ¯”å¦‚æ•°æ®åº“è¿æ¥é”™è¯¯ç­‰ï¼‰
            try:
                # å°è¯•æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥ï¼Œä½†ä¸å›æ»šä¹‹å‰å¯èƒ½å·²ç»æˆåŠŸçš„æ“ä½œ
                job = db.query(SDTMJob).filter(SDTMJob.id == job_id).first()
                if job:
                    job.status = "failed"
                    job.error_message = str(e)
                    db.commit()
                    logger.info(f"å·²å°†ä»»åŠ¡çŠ¶æ€æ›´æ–°ä¸ºå¤±è´¥: {job_id}")
                else:
                    logger.warning(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡è®°å½•: {job_id}")
            except Exception as status_update_error:
                logger.warning(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™: {status_update_error}")
                # å¦‚æœä»»åŠ¡çŠ¶æ€æ›´æ–°å¤±è´¥ï¼Œè¯´æ˜æ•°æ®åº“æœ‰ä¸¥é‡é—®é¢˜ï¼Œæ­¤æ—¶æ‰å›æ»š
                logger.warning("æ•°æ®åº“çŠ¶æ€æœ‰é—®é¢˜ï¼Œæ‰§è¡Œå›æ»šæ“ä½œ")
                db.rollback()
                
                # å†æ¬¡å°è¯•æ›´æ–°ä»»åŠ¡çŠ¶æ€
                try:
                    job = db.query(SDTMJob).filter(SDTMJob.id == job_id).first()
                    if job:
                        job.status = "failed"
                        job.error_message = f"ä¸¥é‡é”™è¯¯: {str(e)} | çŠ¶æ€æ›´æ–°é”™è¯¯: {str(status_update_error)}"
                        db.commit()
                except:
                    logger.error("æ— æ³•æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œæ•°æ®åº“å¯èƒ½æœ‰ä¸¥é‡é—®é¢˜")
            
            logger.info(f"SDTMä»»åŠ¡å¤„ç†å®Œæˆï¼Œå³ä½¿å‘ç”Ÿäº†é”™è¯¯: {job_id}")
        finally:
            logger.debug(f"å…³é—­æ•°æ®åº“ä¼šè¯: {job_id}")
            db.close()
    
    def get_sdtm_job_status(self, job_id: str) -> Optional[SDTMJob]:
        """è·å–SDTMä»»åŠ¡çŠ¶æ€"""
        job = self.db.query(SDTMJob).filter(SDTMJob.id == job_id).first()
        if not job:
            return None

        # å¦‚æœä»»åŠ¡æœ‰task_idï¼Œæ£€æŸ¥é˜Ÿåˆ—ä¸­çš„çŠ¶æ€
        if hasattr(job, 'task_id') and job.task_id:
            from app.utils.task_queue import task_queue
            queue_task = task_queue.get_task_status(job.task_id)
            if queue_task:
                # åŒæ­¥é˜Ÿåˆ—çŠ¶æ€åˆ°æ•°æ®åº“
                from app.utils.task_queue import TaskStatus
                if queue_task.status == TaskStatus.RUNNING and job.status == "pending":
                    job.status = "processing"
                    self.db.commit()
                elif queue_task.status == TaskStatus.COMPLETED and job.status in ["pending", "processing"]:
                    job.status = "completed"
                    self.db.commit()
                elif queue_task.status in [TaskStatus.FAILED, TaskStatus.TIMEOUT] and job.status in ["pending", "processing"]:
                    job.status = "failed"
                    if queue_task.error:
                        job.error_message = queue_task.error
                    self.db.commit()

        return job
    
    def get_kb_sdtm_jobs(self, kb_id: str) -> List[SDTMJob]:
        """è·å–çŸ¥è¯†åº“çš„æ‰€æœ‰SDTMä»»åŠ¡"""
        return self.db.query(SDTMJob).filter(
            SDTMJob.kb_id == kb_id
        ).order_by(SDTMJob.created_at.desc()).all()
    
    def _make_result_serializable(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å°†ç»“æœè½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼"""
        from datetime import datetime, date
        
        def _convert_value(value):
            """é€’å½’è½¬æ¢å•ä¸ªå€¼"""
            # å¤„ç†datetimeå¯¹è±¡
            if isinstance(value, datetime):
                return value.isoformat()
            elif isinstance(value, date):
                return value.isoformat()
            # å¤„ç†è‡ªå®šä¹‰å¯¹è±¡
            elif hasattr(value, '__dict__') and hasattr(value, '__class__'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯Pydanticæ¨¡å‹
                if hasattr(value, 'dict'):
                    try:
                        return _convert_value(value.dict())
                    except Exception as e:
                        print(f"Error converting Pydantic model to dict: {e}")
                        return str(value)
                # æ£€æŸ¥æ˜¯å¦æ˜¯SQLAlchemyæ¨¡å‹æˆ–å…¶ä»–å¯¹è±¡
                else:
                    try:
                        # å°è¯•è½¬æ¢ä¸ºå­—å…¸
                        if hasattr(value, '__table__'):  # SQLAlchemyæ¨¡å‹
                            obj_dict = {}
                            for column in value.__table__.columns:
                                col_value = getattr(value, column.name, None)
                                obj_dict[column.name] = _convert_value(col_value)
                            return obj_dict
                        else:
                            # é€šç”¨å¯¹è±¡è½¬æ¢
                            obj_dict = {}
                            for attr_name in dir(value):
                                if not attr_name.startswith('_') and not callable(getattr(value, attr_name)):
                                    try:
                                        attr_value = getattr(value, attr_name)
                                        obj_dict[attr_name] = _convert_value(attr_value)
                                    except:
                                        continue
                            return obj_dict if obj_dict else str(value)
                    except Exception as e:
                        print(f"Error converting object {type(value)}: {e}")
                        return str(value)
            # å¤„ç†å­—å…¸
            elif isinstance(value, dict):
                return {k: _convert_value(v) for k, v in value.items()}
            # å¤„ç†åˆ—è¡¨
            elif isinstance(value, list):
                return [_convert_value(item) for item in value]
            # å¤„ç†å…ƒç»„
            elif isinstance(value, tuple):
                return [_convert_value(item) for item in value]
            # å¤„ç†é›†åˆ
            elif isinstance(value, set):
                return [_convert_value(item) for item in value]
            # åŸºç¡€ç±»å‹ç›´æ¥è¿”å›
            elif isinstance(value, (str, int, float, bool, type(None))):
                return value
            # å…¶ä»–ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²
            else:
                return str(value)
        
        if not isinstance(result, dict):
            return _convert_value(result)
        
        serializable_result = {}
        for key, value in result.items():
            try:
                # ç‰¹æ®Šå¤„ç†å·²çŸ¥çš„å¤æ‚å¯¹è±¡ç±»å‹
                if key == "operations" and isinstance(value, list):
                    # è½¬æ¢EditOperationå¯¹è±¡ä¸ºå­—å…¸
                    serializable_result[key] = [
                        {
                            "position": op.position if hasattr(op, 'position') else str(op),
                            "payload": _convert_value(op.payload) if hasattr(op, 'payload') else _convert_value(op)
                        } if hasattr(op, 'position') else _convert_value(op)
                        for op in value
                    ]
                elif key == "annotations" and isinstance(value, list):
                    # è½¬æ¢DocumentAnnotationå¯¹è±¡ä¸ºå­—å…¸
                    serializable_result[key] = [
                        {
                            "doc_id": ann.doc_id if hasattr(ann, 'doc_id') else str(ann),
                            "tags": ann.tags if hasattr(ann, 'tags') else [],
                            "confidence": ann.confidence if hasattr(ann, 'confidence') else 0.0
                        } if hasattr(ann, 'doc_id') else _convert_value(ann)
                        for ann in value
                    ]
                else:
                    # ä½¿ç”¨é€šç”¨è½¬æ¢æ–¹æ³•
                    serializable_result[key] = _convert_value(value)
                    
            except Exception as e:
                print(f"Error serializing key '{key}' with value type {type(value)}: {e}")
                serializable_result[key] = str(value)
        
        return serializable_result
    
    def test_serialization(self, test_data: Any) -> bool:
        """æµ‹è¯•æ•°æ®æ˜¯å¦å¯ä»¥æˆåŠŸåºåˆ—åŒ–"""
        try:
            serializable_data = self._make_result_serializable(test_data)
            json.dumps(serializable_data, ensure_ascii=False)
            return True
        except Exception as e:
            print(f"åºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def process_knowledge_base(
        self,
        kb_id: str,
        mode: SDTMMode,
        batch_size: int = 10,
        max_iterations: int = 100,
        auto_apply: bool = True,
        abnormal_doc_slots: int = 3,
        normal_doc_slots: int = 7,
        abnormal_doc_threshold: float = 3.0,
        enable_early_termination: bool = True
    ) -> Dict[str, Any]:
        """å¤„ç†çŸ¥è¯†åº“ - ä¸»è¦å…¥å£ç‚¹"""
        
        try:
            # è·å–çŸ¥è¯†åº“ä¿¡æ¯
            kb = self.kb_service.get_kb_by_id(kb_id)
            if not kb:
                raise ValueError(f"Knowledge base {kb_id} not found")
            
            # è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯
            stats = self.stats_service.get_kb_sdtm_stats(kb_id)
            
            # è·å–å¾…å¤„ç†æ–‡æ¡£
            documents_to_process = self.stats_service.get_documents_to_process(kb_id, batch_size)
            
            if not documents_to_process:
                return {
                    "success": True,
                    "message": "æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æ¡£",
                    "operations": [],
                    "annotations": [],
                    "stats": stats
                }
            
            # è°ƒç”¨SDTMå¼•æ“å¤„ç†æ–‡æ¡£
            response = await self.engine.process_documents(
                mode=mode,
                progress_metrics=stats.progress_metrics,
                quality_metrics=stats.quality_metrics,
                current_tag_dictionary=kb.tag_dictionary or {},
                documents_to_process=documents_to_process,
                abnormal_documents=stats.abnormal_documents
            )
            
            # æ ¹æ®æ¨¡å¼å¤„ç†å“åº”
            if mode == SDTMMode.EDIT:
                return await self._handle_edit_mode(kb_id, response, auto_apply)
            elif mode == SDTMMode.ANNOTATE:
                return await self._handle_annotate_mode(kb_id, response, auto_apply)
            elif mode == SDTMMode.SHADOW:
                return await self._handle_shadow_mode(kb_id, response)
            
        except Exception as e:
            return {
                "success": False,
                "message": f"å¤„ç†å¤±è´¥: {str(e)}",
                "operations": [],
                "annotations": [],
                "error": str(e)
            }
    
    async def _handle_edit_mode(
        self, 
        kb_id: str, 
        response: SDTMEngineResponse, 
        auto_apply: bool = True
    ) -> Dict[str, Any]:
        """å¤„ç†ç¼–è¾‘æ¨¡å¼"""
        
        kb = self.kb_service.get_kb_by_id(kb_id)
        current_dict = kb.tag_dictionary or {}
        
        # é¢„è§ˆç¼–è¾‘æ“ä½œæ•ˆæœ
        preview_dict = self.engine.preview_edit_operations(current_dict, response.operations)
        
        applied_operations = []
        if auto_apply and response.operations:
            # æ£€æŸ¥å¼•æ“æ˜¯å¦å·²ç»åº”ç”¨äº†æ“ä½œ
            if response.updated_dictionary is not None:
                # ä½¿ç”¨å¼•æ“å·²ç»åº”ç”¨çš„å­—å…¸
                try:
                    logger.info(f"ä½¿ç”¨å¼•æ“å·²æ›´æ–°çš„æ ‡ç­¾å­—å…¸ (åŒ…å« {len(response.operations)} ä¸ªæ“ä½œ)")
                    logger.debug(f"æ›´æ–°å‰çš„å­—å…¸: {current_dict}")
                    logger.debug(f"æ›´æ–°åçš„å­—å…¸: {response.updated_dictionary}")
                    
                    # æ›´æ–°çŸ¥è¯†åº“æ ‡ç­¾å­—å…¸
                    from app.schemas.knowledge_base import TagDictionaryUpdate
                    tag_update = TagDictionaryUpdate(tag_dictionary=response.updated_dictionary)
                    
                    logger.info("å¼€å§‹ä¿å­˜æ ‡ç­¾å­—å…¸åˆ°æ•°æ®åº“...")
                    updated_kb = self.kb_service.update_tag_dictionary(kb_id, tag_update)
                    logger.info(f"æ ‡ç­¾å­—å…¸å·²ä¿å­˜ï¼Œæ›´æ–°æ—¶é—´: {updated_kb.last_tag_directory_update_time}")
                    
                    applied_operations = response.operations
                    
                except Exception as e:
                    logger.error(f"ä½¿ç”¨å¼•æ“æ›´æ–°å­—å…¸æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            else:
                # å›é€€åˆ°æ‰‹åŠ¨åº”ç”¨æ“ä½œ
                try:
                    logger.debug("æ‰‹åŠ¨åº”ç”¨ç¼–è¾‘æ“ä½œ")
                    new_dict = self.engine.apply_edit_operations(current_dict, response.operations)
                    
                    # æ›´æ–°çŸ¥è¯†åº“æ ‡ç­¾å­—å…¸
                    from app.schemas.knowledge_base import TagDictionaryUpdate
                    tag_update = TagDictionaryUpdate(tag_dictionary=new_dict)
                    self.kb_service.update_tag_dictionary(kb_id, tag_update)
                    
                    applied_operations = response.operations
                    
                except Exception as e:
                    logger.error(f"æ‰‹åŠ¨åº”ç”¨ç¼–è¾‘æ“ä½œæ—¶å‡ºé”™: {e}")
        
        # åº”ç”¨æ–‡æ¡£æ ‡æ³¨
        applied_annotations = []
        if auto_apply and response.annotations:
            applied_annotations = await self._apply_annotations(response.annotations)
        
        # è·å–æ›´æ–°åçš„ç»Ÿè®¡ä¿¡æ¯
        updated_stats = self.stats_service.get_kb_sdtm_stats(kb_id)
        
        return {
            "success": True,
            "message": f"ç¼–è¾‘æ¨¡å¼å¤„ç†å®Œæˆï¼Œåº”ç”¨äº† {len(applied_operations)} ä¸ªæ“ä½œï¼Œ{len(applied_annotations)} ä¸ªæ ‡æ³¨",
            "operations": applied_operations,
            "annotations": applied_annotations,
            "preview_dictionary": preview_dict,
            "reasoning": response.reasoning,
            "stats": updated_stats
        }
    
    async def _handle_annotate_mode(
        self, 
        kb_id: str, 
        response: SDTMEngineResponse, 
        auto_apply: bool = True
    ) -> Dict[str, Any]:
        """å¤„ç†æ ‡æ³¨æ¨¡å¼"""
        
        applied_annotations = []
        if auto_apply and response.annotations:
            applied_annotations = await self._apply_annotations(response.annotations)
        
        # åœ¨æ ‡æ³¨æ¨¡å¼ä¸‹ï¼Œåªåº”ç”¨å°‘é‡å¿…è¦çš„ç¼–è¾‘æ“ä½œ
        applied_operations = []
        if auto_apply and response.operations:
            # æ£€æŸ¥å¼•æ“æ˜¯å¦å·²ç»åº”ç”¨äº†æ“ä½œ
            if response.updated_dictionary is not None:
                # ä½¿ç”¨å¼•æ“å·²ç»åº”ç”¨çš„å­—å…¸ï¼Œä½†åœ¨æ ‡æ³¨æ¨¡å¼ä¸‹éœ€è¦è°¨æ…
                try:
                    logger.info(f"æ ‡æ³¨æ¨¡å¼ï¼šä½¿ç”¨å¼•æ“å·²æ›´æ–°çš„æ ‡ç­¾å­—å…¸ (åŒ…å« {len(response.operations)} ä¸ªæ“ä½œ)")
                    
                    # æ›´æ–°çŸ¥è¯†åº“æ ‡ç­¾å­—å…¸
                    from app.schemas.knowledge_base import TagDictionaryUpdate
                    tag_update = TagDictionaryUpdate(tag_dictionary=response.updated_dictionary)
                    
                    logger.debug("æ ‡æ³¨æ¨¡å¼ï¼šå¼€å§‹ä¿å­˜æ ‡ç­¾å­—å…¸åˆ°æ•°æ®åº“...")
                    updated_kb = self.kb_service.update_tag_dictionary(kb_id, tag_update)
                    logger.info(f"æ ‡æ³¨æ¨¡å¼ï¼šæ ‡ç­¾å­—å…¸å·²ä¿å­˜ï¼Œæ›´æ–°æ—¶é—´: {updated_kb.last_tag_directory_update_time}")
                    
                    applied_operations = response.operations
                    
                except Exception as e:
                    logger.error(f"æ ‡æ³¨æ¨¡å¼ï¼šä½¿ç”¨å¼•æ“æ›´æ–°å­—å…¸æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(f"æ ‡æ³¨æ¨¡å¼ï¼šè¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            else:
                # å›é€€åˆ°æ‰‹åŠ¨åº”ç”¨æ“ä½œï¼Œåªåº”ç”¨å‰3ä¸ªæ“ä½œï¼Œé¿å…è¿‡åº¦ä¿®æ”¹
                limited_operations = response.operations[:3]
                
                if limited_operations:
                    kb = self.kb_service.get_kb_by_id(kb_id)
                    current_dict = kb.tag_dictionary or {}
                    
                    try:
                        logger.debug(f"æ ‡æ³¨æ¨¡å¼ï¼šæ‰‹åŠ¨åº”ç”¨æœ‰é™çš„ç¼–è¾‘æ“ä½œ ({len(limited_operations)} ä¸ª)")
                        new_dict = self.engine.apply_edit_operations(current_dict, limited_operations)
                        
                        from app.schemas.knowledge_base import TagDictionaryUpdate
                        tag_update = TagDictionaryUpdate(tag_dictionary=new_dict)
                        self.kb_service.update_tag_dictionary(kb_id, tag_update)
                        
                        applied_operations = limited_operations
                        
                    except Exception as e:
                        logger.error(f"æ ‡æ³¨æ¨¡å¼ï¼šæ‰‹åŠ¨åº”ç”¨æœ‰é™ç¼–è¾‘æ“ä½œæ—¶å‡ºé”™: {e}")
        
        # è·å–æ›´æ–°åçš„ç»Ÿè®¡ä¿¡æ¯
        updated_stats = self.stats_service.get_kb_sdtm_stats(kb_id)
        
        return {
            "success": True,
            "message": f"æ ‡æ³¨æ¨¡å¼å¤„ç†å®Œæˆï¼Œåº”ç”¨äº† {len(applied_annotations)} ä¸ªæ ‡æ³¨ï¼Œ{len(applied_operations)} ä¸ªç¼–è¾‘æ“ä½œ",
            "operations": applied_operations,
            "annotations": applied_annotations,
            "reasoning": response.reasoning,
            "stats": updated_stats
        }
    
    async def _handle_shadow_mode(
        self, 
        kb_id: str, 
        response: SDTMEngineResponse
    ) -> Dict[str, Any]:
        """å¤„ç†å½±å­æ¨¡å¼ - ç”¨äºç›‘æµ‹è¯­ä¹‰æ¼‚ç§»"""
        
        # åœ¨å½±å­æ¨¡å¼ä¸‹ï¼Œä¸åº”ç”¨ä»»ä½•æ“ä½œï¼Œåªæ˜¯ç›‘æµ‹å’Œè®°å½•
        kb = self.kb_service.get_kb_by_id(kb_id)
        current_dict = kb.tag_dictionary or {}
        
        # ä½¿ç”¨å¼•æ“é¢„è§ˆçš„å­—å…¸ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–æ‰‹åŠ¨é¢„è§ˆæ“ä½œæ•ˆæœ
        if response.updated_dictionary is not None:
            preview_dict = response.updated_dictionary
            logger.debug("å½±å­æ¨¡å¼ï¼šä½¿ç”¨å¼•æ“é¢„è§ˆçš„å­—å…¸")
        else:
            preview_dict = self.engine.preview_edit_operations(current_dict, response.operations)
            logger.debug("å½±å­æ¨¡å¼ï¼šæ‰‹åŠ¨é¢„è§ˆæ“ä½œæ•ˆæœ")
        
        # è®¡ç®—è¯­ä¹‰æ¼‚ç§»æŒ‡æ ‡
        drift_metrics = self._calculate_semantic_drift(current_dict, preview_dict, response)
        
        # è®°å½•å½±å­æ¨¡å¼æ“ä½œï¼ˆå¯ä»¥å­˜å‚¨åˆ°æ—¥å¿—æˆ–æ•°æ®åº“ï¼‰
        shadow_log = {
            "kb_id": kb_id,
            "timestamp": datetime.now().isoformat(),
            "operations": [op.dict() for op in response.operations],
            "annotations": [ann.dict() for ann in response.annotations],
            "drift_metrics": drift_metrics,
            "reasoning": response.reasoning
        }
        
        # è¿™é‡Œå¯ä»¥å­˜å‚¨åˆ°ç›‘æ§ç³»ç»Ÿæˆ–æ—¥å¿—
        print(f"Shadow mode log: {json.dumps(shadow_log, ensure_ascii=False, indent=2)}")
        
        return {
            "success": True,
            "message": f"å½±å­æ¨¡å¼å¤„ç†å®Œæˆï¼Œæ£€æµ‹åˆ° {len(response.operations)} ä¸ªæ½œåœ¨ç¼–è¾‘æ“ä½œ",
            "operations": response.operations,  # ä¸åº”ç”¨ï¼Œåªè¿”å›
            "annotations": response.annotations,  # ä¸åº”ç”¨ï¼Œåªè¿”å›
            "preview_dictionary": preview_dict,
            "drift_metrics": drift_metrics,
            "reasoning": response.reasoning,
            "shadow_log": shadow_log
        }
    
    def _calculate_semantic_drift(
        self, 
        current_dict: Dict[str, Any], 
        preview_dict: Dict[str, Any], 
        response: SDTMEngineResponse
    ) -> Dict[str, Any]:
        """è®¡ç®—è¯­ä¹‰æ¼‚ç§»æŒ‡æ ‡"""
        
        # ç®€å•çš„æ¼‚ç§»æŒ‡æ ‡è®¡ç®—
        current_tags = self._flatten_dictionary(current_dict)
        preview_tags = self._flatten_dictionary(preview_dict)
        
        added_tags = set(preview_tags) - set(current_tags)
        removed_tags = set(current_tags) - set(preview_tags)
        
        return {
            "operations_count": len(response.operations),
            "annotations_count": len(response.annotations),
            "added_tags_count": len(added_tags),
            "removed_tags_count": len(removed_tags),
            "total_tags_before": len(current_tags),
            "total_tags_after": len(preview_tags),
            "drift_percentage": (len(added_tags) + len(removed_tags)) / max(len(current_tags), 1) * 100
        }
    
    def _flatten_dictionary(self, d: Dict[str, Any]) -> List[str]:
        """å±•å¹³å­—å…¸ï¼Œè·å–æ‰€æœ‰å¶å­èŠ‚ç‚¹æ ‡ç­¾"""
        tags = []
        for key, value in d.items():
            if isinstance(value, dict):
                tags.extend(self._flatten_dictionary(value))
            elif isinstance(value, list):
                tags.extend(value)
            else:
                tags.append(key)
        return tags
    
    async def _apply_annotations(self, annotations: List[DocumentAnnotation]) -> List[DocumentAnnotation]:
        """åº”ç”¨æ–‡æ¡£æ ‡æ³¨ - åŒæ—¶æ›´æ–°SQLiteå’ŒMilvusè®°å½•"""
        applied_annotations = []
        
        for annotation in annotations:
            try:
                # æ›´æ–°SQLiteä¸­çš„chunkæ ‡ç­¾
                chunk = self.chunk_repo.get_chunk_by_id(annotation.doc_id)
                if chunk:
                    # æ›´æ–°chunkçš„æ ‡ç­¾å­—æ®µ
                    old_tags = chunk.tags
                    chunk.tags = json.dumps(annotation.tags, ensure_ascii=False)
                    self.db.commit()
                    
                    # åŒæ—¶æ›´æ–°Milvusä¸­çš„å‘é‡è®°å½•
                    try:
                        from app.repositories.milvus_repo import MilvusRepository
                        milvus_repo = MilvusRepository()
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å‘é‡è®°å½•
                        vector_exists = milvus_repo.check_vector_exists(chunk.kb_id, chunk.id)
                        
                        if vector_exists:
                            # æ›´æ–°Milvusä¸­çš„æ ‡ç­¾å…ƒæ•°æ®
                            milvus_repo.update_vector_metadata(
                                kb_id=chunk.kb_id,
                                chunk_id=chunk.id,
                                metadata={
                                    "tags": annotation.tags,
                                    "last_updated": datetime.now().isoformat()
                                }
                            )
                            logger.debug(f"Updated vector metadata for chunk {chunk.id}")
                        else:
                            logger.warning(f"No vector found for chunk {chunk.id} in Milvus")
                            
                    except Exception as milvus_error:
                        logger.error(f"Error updating Milvus for chunk {chunk.id}: {milvus_error}")
                        # Milvusæ›´æ–°å¤±è´¥ä¸åº”é˜»æ­¢SQLiteæ›´æ–°æˆåŠŸçš„è®°å½•
                    
                    applied_annotations.append(annotation)
                    logger.debug(f"Successfully applied annotation to chunk {annotation.doc_id}: {old_tags} -> {annotation.tags}")
                else:
                    logger.warning(f"Chunk {annotation.doc_id} not found for annotation")
                    
            except Exception as e:
                logger.error(f"Error applying annotation to {annotation.doc_id}: {e}")
        
        return applied_annotations
    
    async def optimize_tag_dictionary(
        self, 
        kb_id: str, 
        batch_size: int = 10, 
        auto_apply: bool = False
    ) -> Dict[str, Any]:
        """ä¼˜åŒ–æ ‡ç­¾å­—å…¸ - ä¸“é—¨ç”¨äºå­—å…¸ä¼˜åŒ–çš„æ¥å£"""
        
        return await self.process_knowledge_base(
            kb_id=kb_id,
            mode=SDTMMode.EDIT,
            batch_size=batch_size,
            auto_apply=auto_apply
        )
    
    async def batch_annotate_documents(
        self, 
        kb_id: str, 
        document_ids: List[str] = None,
        batch_size: int = 10
    ) -> Dict[str, Any]:
        """æ‰¹é‡æ ‡æ³¨æ–‡æ¡£"""
        
        # å¦‚æœæŒ‡å®šäº†æ–‡æ¡£IDï¼Œåˆ™åªå¤„ç†è¿™äº›æ–‡æ¡£
        if document_ids:
            documents_to_process = []
            for doc_id in document_ids:
                chunk = self.chunk_repo.get_chunk_by_id(doc_id)
                if chunk:
                    try:
                        current_tags = []
                        if chunk.tags:
                            if isinstance(chunk.tags, str):
                                current_tags = json.loads(chunk.tags)
                            else:
                                current_tags = chunk.tags
                        
                        documents_to_process.append(DocumentInfo(
                            doc_id=chunk.id,
                            content=chunk.content,
                            current_tags=current_tags,
                            kb_id=kb_id,
                            chunk_index=chunk.chunk_index
                        ))
                    except (json.JSONDecodeError, TypeError):
                        documents_to_process.append(DocumentInfo(
                            doc_id=chunk.id,
                            content=chunk.content,
                            current_tags=[],
                            kb_id=kb_id,
                            chunk_index=chunk.chunk_index
                        ))
            
            # ç›´æ¥å¤„ç†æŒ‡å®šçš„æ–‡æ¡£
            if documents_to_process:
                kb = self.kb_service.get_kb_by_id(kb_id)
                stats = self.stats_service.get_kb_sdtm_stats(kb_id)
                
                response = await self.engine.process_documents(
                    mode=SDTMMode.ANNOTATE,
                    progress_metrics=stats.progress_metrics,
                    quality_metrics=stats.quality_metrics,
                    current_tag_dictionary=kb.tag_dictionary or {},
                    documents_to_process=documents_to_process,
                    abnormal_documents=stats.abnormal_documents
                )
                
                # åº”ç”¨æ ‡æ³¨
                applied_annotations = await self._apply_annotations(response.annotations)
                
                return {
                    "success": True,
                    "message": f"æ‰¹é‡æ ‡æ³¨å®Œæˆï¼Œå¤„ç†äº† {len(applied_annotations)} ä¸ªæ–‡æ¡£",
                    "annotations": applied_annotations,
                    "reasoning": response.reasoning,
                    "failed_documents": [doc_id for doc_id in document_ids 
                                       if doc_id not in [ann.doc_id for ann in applied_annotations]]
                }
        
        # å¦åˆ™ä½¿ç”¨æ ‡å‡†çš„æ ‡æ³¨æ¨¡å¼
        return await self.process_knowledge_base(
            kb_id=kb_id,
            mode=SDTMMode.ANNOTATE,
            batch_size=batch_size
        )
    
    def get_kb_stats(self, kb_id: str) -> SDTMStats:
        """è·å–çŸ¥è¯†åº“SDTMç»Ÿè®¡ä¿¡æ¯"""
        return self.stats_service.get_kb_sdtm_stats(kb_id)
    
    def _verify_tag_dictionary_update(self, db: Session, kb_id: str, job_id: str):
        """éªŒè¯æ ‡ç­¾å­—å…¸æ˜¯å¦ç¡®å®è¢«æ›´æ–°åˆ°æ•°æ®åº“"""
        try:
            logger.debug("éªŒè¯æ ‡ç­¾å­—å…¸æ›´æ–°çŠ¶æ€...")
            from app.models.knowledge_base import KnowledgeBase
            kb = db.query(KnowledgeBase).filter(KnowledgeBase.id == kb_id).first()
            if kb:
                logger.debug(f"éªŒè¯æˆåŠŸ: çŸ¥è¯†åº“ {kb_id} å·²æ‰¾åˆ°")
                logger.debug(f"   æ ‡ç­¾å­—å…¸å­˜åœ¨: {kb.tag_dictionary is not None}")
                if kb.tag_dictionary:
                    dict_size = len(str(kb.tag_dictionary))
                    logger.debug(f"   æ ‡ç­¾å­—å…¸å¤§å°: {dict_size} å­—ç¬¦")
                    
                    # æ˜¾ç¤ºå­—å…¸çš„é¡¶çº§ç»“æ„
                    if isinstance(kb.tag_dictionary, dict):
                        top_keys = list(kb.tag_dictionary.keys())[:5]  # æ˜¾ç¤ºå‰5ä¸ªé”®
                        logger.debug(f"   é¡¶çº§é”®: {top_keys}")
                else:
                    logger.warning("   æ ‡ç­¾å­—å…¸ä¸ºç©º")
                
                logger.debug(f"   æœ€åæ›´æ–°æ—¶é—´: {kb.last_tag_directory_update_time}")
                
                if kb.last_tag_directory_update_time:
                    from datetime import datetime, timedelta
                    now = datetime.now()
                    time_diff = now - kb.last_tag_directory_update_time
                    if time_diff < timedelta(minutes=5):
                        logger.debug(f"   æ›´æ–°æ—¶é—´æ­£å¸¸ï¼Œè·ç¦»ç°åœ¨ {time_diff.total_seconds():.1f} ç§’")
                    else:
                        logger.warning(f"   æ›´æ–°æ—¶é—´è¾ƒæ—©ï¼Œè·ç¦»ç°åœ¨ {time_diff}")
                else:
                    logger.warning("   æœªè®°å½•æ›´æ–°æ—¶é—´")
            else:
                logger.error(f"éªŒè¯å¤±è´¥: çŸ¥è¯†åº“ {kb_id} æœªæ‰¾åˆ°")
        except Exception as e:
            logger.error(f"éªŒè¯æ ‡ç­¾å­—å…¸æ›´æ–°æ—¶å‡ºé”™: {e}")
            import traceback
            logger.error(f"éªŒè¯é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}") 