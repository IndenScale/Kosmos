#!/usr/bin/env python3
"""
æ–‡æ¡£æ‘„å…¥æµæ°´çº¿æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•æ–‡æ¡£å¤„ç†å’Œchunkåˆ†å‰²æ•ˆæœï¼Œä¸è¿›è¡ŒæŒä¹…åŒ–æ“ä½œ
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.processors.processor_factory import ProcessorFactory
from app.utils.intelligent_text_splitter import IntelligentTextSplitter
from app.utils.ai_utils import AIUtils


class IngestionPipelineTester:
    """æ‘„å…¥æµæ°´çº¿æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.processor_factory = ProcessorFactory()
        self.intelligent_splitter = IntelligentTextSplitter()
        self.ai_utils = AIUtils()
        
    def test_document_processing(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•æ–‡æ¡£å¤„ç†æµç¨‹"""
        print(f"ğŸ” æµ‹è¯•æ–‡æ¡£: {file_path}")
        
        if not os.path.exists(file_path):
            return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        try:
            # 1. è·å–å¤„ç†å™¨
            processor = self.processor_factory.get_processor(file_path)
            print(f"ğŸ“‹ ä½¿ç”¨å¤„ç†å™¨: {processor.__class__.__name__}")
            
            # 2. æå–å†…å®¹ï¼ˆå¯èƒ½æ˜¯ç»“æ„åŒ–å—æˆ–çº¯æ–‡æœ¬ï¼‰
            print("ğŸ”„ æå–æ–‡æ¡£å†…å®¹...")
            raw_content, screenshot_paths = processor.extract_content(file_path)
            
            # 3. å°†å†…å®¹ç»Ÿä¸€ä¸ºç»“æ„åŒ–å—åˆ—è¡¨
            if isinstance(raw_content, str):
                content_blocks = [{'type': 'text', 'content': raw_content}]
                original_content_length = len(raw_content)
            elif isinstance(raw_content, list):
                content_blocks = raw_content
                # ä¼°ç®—åŸå§‹å†…å®¹é•¿åº¦
                original_content_length = sum(len(self.intelligent_splitter._format_block_to_markdown(b)) for b in content_blocks)
            else:
                raise TypeError(f"ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {type(raw_content)}")

            # 4. åˆ†æåŸå§‹å†…å®¹
            print(f"ğŸ“„ åŸå§‹å†…å®¹é•¿åº¦ (ä¼°ç®—): {original_content_length} å­—ç¬¦")
            print(f"ğŸ§± å†…å®¹å—æ•°é‡: {len(content_blocks)}")
            print(f"ğŸ“¸ ç”Ÿæˆæˆªå›¾æ•°é‡: {len(screenshot_paths)}")
            
            # 5. ä½¿ç”¨æ™ºèƒ½åˆ†å‰²å™¨åˆ†å‰²æ–‡æœ¬
            print("âœ‚ï¸ ä½¿ç”¨æ™ºèƒ½åˆ†å‰²å™¨åˆ†å‰²æ–‡æœ¬ä¸ºchunks...")
            chunks = self.intelligent_splitter.split(content_blocks)
            print(f"ğŸ“¦ åˆ†å‰²åchunksæ•°é‡: {len(chunks)}")
            
            # 6. åˆ†æchunksè´¨é‡
            chunk_analysis = self._analyze_chunks(chunks)
            
            # 7. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            report = {
                "file_path": file_path,
                "processor": processor.__class__.__name__,
                "original_content_length": original_content_length,
                "block_count": len(content_blocks),
                "screenshot_count": len(screenshot_paths),
                "chunk_count": len(chunks),
                "chunk_analysis": chunk_analysis,
                "sample_chunks": self._get_sample_chunks(chunks, 3)
            }
            
            return report
            
        except Exception as e:
            return {"error": f"å¤„ç†å¤±è´¥: {str(e)}"}
    
    def _analyze_chunks(self, chunks: List[str]) -> Dict[str, Any]:
        """åˆ†æchunksè´¨é‡"""
        if not chunks:
            return {"total": 0, "empty": 0, "short": 0, "normal": 0, "long": 0}
        
        empty_chunks = 0
        short_chunks = 0  # < 50å­—ç¬¦
        normal_chunks = 0  # 50-1000å­—ç¬¦
        long_chunks = 0  # > 1000å­—ç¬¦
        
        length_distribution = []
        
        for chunk in chunks:
            content = chunk.strip()
            length = len(content)
            length_distribution.append(length)
            
            if length == 0:
                empty_chunks += 1
            elif length < 50:
                short_chunks += 1
            elif length <= 1000:
                normal_chunks += 1
            else:
                long_chunks += 1
        
        avg_length = sum(length_distribution) / len(length_distribution)
        
        return {
            "total": len(chunks),
            "empty": empty_chunks,
            "short": short_chunks,
            "normal": normal_chunks,
            "long": long_chunks,
            "average_length": round(avg_length, 2),
            "min_length": min(length_distribution),
            "max_length": max(length_distribution),
            "quality_score": self._calculate_quality_score(empty_chunks, short_chunks, normal_chunks, long_chunks)
        }
    
    def _calculate_quality_score(self, empty: int, short: int, normal: int, long: int) -> float:
        """è®¡ç®—chunksè´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        total = empty + short + normal + long
        if total == 0:
            return 0.0
        
        # è´¨é‡åˆ†æ•°è®¡ç®—ï¼š
        # - ç©ºchunk: -10åˆ†
        # - çŸ­chunk: -5åˆ†
        # - æ­£å¸¸chunk: +10åˆ†
        # - é•¿chunk: +5åˆ†
        score = (normal * 10 + long * 5 - short * 5 - empty * 10) / total
        
        # å½’ä¸€åŒ–åˆ°0-100èŒƒå›´
        normalized_score = max(0, min(100, (score + 10) * 5))
        
        return round(normalized_score, 2)
    
    def _get_sample_chunks(self, chunks: List[str], count: int = 3) -> List[Dict[str, Any]]:
        """è·å–æ ·æœ¬chunks"""
        if not chunks:
            return []
        
        sample_chunks = []
        
        # é€‰æ‹©ä¸åŒä½ç½®çš„chunksä½œä¸ºæ ·æœ¬
        indices = []
        if len(chunks) >= count:
            indices = [0, len(chunks) // 2, len(chunks) - 1]
        else:
            indices = list(range(len(chunks)))
        
        for i in indices[:count]:
            chunk = chunks[i]
            sample_chunks.append({
                "index": i,
                "length": len(chunk),
                "preview": chunk[:200] + "..." if len(chunk) > 200 else chunk,
                "is_valid": len(chunk.strip()) >= 50  # åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆchunk
            })
        
        return sample_chunks
    
    def print_report(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        if "error" in report:
            print(f"âŒ é”™è¯¯: {report['error']}")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š æ–‡æ¡£å¤„ç†æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {report['file_path']}")
        print(f"ğŸ”§ å¤„ç†å™¨: {report['processor']}")
        print(f"ğŸ“„ åŸå§‹å†…å®¹é•¿åº¦: {report['original_content_length']:,} å­—ç¬¦")
        print(f"ğŸ§± å†…å®¹å—æ•°é‡: {report['block_count']}")
        print(f"ğŸ“¸ æˆªå›¾æ•°é‡: {report['screenshot_count']}")
        print(f"ğŸ“¦ Chunkæ•°é‡: {report['chunk_count']}")
        
        analysis = report['chunk_analysis']
        print(f"\nğŸ“ˆ Chunkè´¨é‡åˆ†æ:")
        print(f"  â€¢ æ€»æ•°: {analysis['total']}")
        print(f"  â€¢ ç©ºchunk: {analysis['empty']} ({analysis['empty']/analysis['total']*100:.1f}%)")
        print(f"  â€¢ çŸ­chunk (<50å­—ç¬¦): {analysis['short']} ({analysis['short']/analysis['total']*100:.1f}%)")
        print(f"  â€¢ æ­£å¸¸chunk (50-1000å­—ç¬¦): {analysis['normal']} ({analysis['normal']/analysis['total']*100:.1f}%)")
        print(f"  â€¢ é•¿chunk (>1000å­—ç¬¦): {analysis['long']} ({analysis['long']/analysis['total']*100:.1f}%)")
        print(f"  â€¢ å¹³å‡é•¿åº¦: {analysis['average_length']:.1f} å­—ç¬¦")
        print(f"  â€¢ é•¿åº¦èŒƒå›´: {analysis['min_length']} - {analysis['max_length']} å­—ç¬¦")
        print(f"  â€¢ è´¨é‡åˆ†æ•°: {analysis['quality_score']}/100")
        
        print(f"\nğŸ“ æ ·æœ¬Chunks:")
        for i, sample in enumerate(report['sample_chunks'], 1):
            status = "âœ… æœ‰æ•ˆ" if sample['is_valid'] else "âŒ æ— æ•ˆ"
            print(f"  {i}. Chunk #{sample['index']} ({sample['length']} å­—ç¬¦) {status}")
            print(f"     é¢„è§ˆ: {sample['preview']}")
            print()
    
    def test_multiple_documents(self, document_dir: str = "data/documents"):
        """æµ‹è¯•å¤šä¸ªæ–‡æ¡£"""
        print(f"ğŸ” æ‰«ææ–‡æ¡£ç›®å½•: {document_dir}")
        
        if not os.path.exists(document_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {document_dir}")
            return
        
        # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        supported_extensions = self.processor_factory.get_supported_extensions()
        files = []
        
        for file_path in Path(document_dir).iterdir():
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                files.append(str(file_path))
        
        if not files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # æµ‹è¯•æ¯ä¸ªæ–‡ä»¶
        for file_path in files:
            print(f"\n{'='*60}")
            report = self.test_document_processing(file_path)
            self.print_report(report)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ–‡æ¡£æ‘„å…¥æµæ°´çº¿æµ‹è¯•")
    
    tester = IngestionPipelineTester()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # æµ‹è¯•æŒ‡å®šæ–‡ä»¶
        file_path = sys.argv[1]
        report = tester.test_document_processing(file_path)
        tester.print_report(report)
    else:
        # æµ‹è¯•æ‰€æœ‰æ–‡æ¡£
        tester.test_multiple_documents()


if __name__ == "__main__":
    main() 